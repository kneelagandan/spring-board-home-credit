{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data wrangling - combining files into one single file (one line of information per each user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-350f56073a21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading the Credit Bureau file from csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbureau\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bureau.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   2429\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2430\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2431\u001b[1;33m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2432\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   3179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3180\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3181\u001b[1;33m                             \u001b[0mnew_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_iter_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3182\u001b[0m                             \u001b[0mrows\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_next_iter_line\u001b[1;34m(self, row_num)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2890\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2893\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_bad_lines\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reading the Credit Bureau file from csv\n",
    "bureau = pd.read_csv('bureau.csv', index_col=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Bureau Balance file from csv\n",
    "bureau_balance = pd.read_csv('bureau_balance.csv', index_col=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = bureau_balance.join(pd.get_dummies(bureau_balance.STATUS)).drop(\"STATUS\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = bureau_balance.drop(\"MONTHS_BALANCE\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = bureau_balance['SK_ID_BUREAU'].value_counts().to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_grouped = bureau_balance.groupby(['SK_ID_BUREAU'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance_grouped['Month_Balance_Count'] = bureau_balance_grouped['SK_ID_BUREAU'].map(counts)\n",
    "bureau_balance_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to the Credit Bureau file - transforming categorical variables into Dummies\n",
    "\n",
    "bureau = bureau.join(pd.get_dummies(bureau.CREDIT_ACTIVE)).drop(\"CREDIT_ACTIVE\", axis=1)\n",
    "bureau = bureau.join(pd.get_dummies(bureau.CREDIT_CURRENCY)).drop(\"CREDIT_CURRENCY\", axis=1)\n",
    "bureau = bureau.join(pd.get_dummies(bureau.CREDIT_TYPE)).drop(\"CREDIT_TYPE\", axis=1)\n",
    "\n",
    "bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Credit Bureau file with the Balance file\n",
    "bureau = bureau.merge(bureau_balance_grouped, on=('SK_ID_BUREAU'), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_grouped = bureau.groupby(['SK_ID_CURR'], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validating unique ID count against the size of final grouped DataFrame\n",
    "bureau_grouped['SK_ID_CURR'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_grouped = bureau_grouped.drop(['SK_ID_BUREAU'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Credit Card Balance file from csv\n",
    "cc_balance = pd.read_csv('credit_card_balance.csv', index_col=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming categorical variable into Dummies\n",
    "\n",
    "cc_balance = cc_balance.join(pd.get_dummies(cc_balance.NAME_CONTRACT_STATUS)).drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "\n",
    "cc_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_balance['SK_ID_PREV'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unique combinations of the previous loans IDs with the current loan IDs\n",
    "cc_balance_prev_curr = cc_balance[['SK_ID_PREV','SK_ID_CURR']]\n",
    "cc_balance_prev_curr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_balance_prev_curr = cc_balance_prev_curr.groupby(['SK_ID_PREV'], as_index=False).mean()\n",
    "cc_balance_prev_curr.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Current ID variable - later will re-merge\n",
    "cc_balance = cc_balance.drop(\"SK_ID_CURR\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing past Installement payment information and grouping by previous ID.\n",
    "cc_balance = cc_balance.groupby(['SK_ID_PREV'], as_index=False).sum()\n",
    "cc_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging back the unique SK_ID_CURR variable to the installments_payments DataFrame\n",
    "cc_balance = cc_balance.merge(cc_balance_prev_curr, on=('SK_ID_PREV'), how='left')\n",
    "cc_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_balance['SK_ID_PREV'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Previous Applications file from csv\n",
    "prev_app = pd.read_csv('previous_application.csv', index_col=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous Applications - transforming categorical variables into Dummies\n",
    "\n",
    "prev_app = prev_app.join(pd.get_dummies(prev_app.NAME_CONTRACT_TYPE)).drop(\"NAME_CONTRACT_TYPE\", axis=1)\n",
    "prev_app = prev_app.join(pd.get_dummies(prev_app.WEEKDAY_APPR_PROCESS_START)).drop(\"WEEKDAY_APPR_PROCESS_START\", axis=1)\n",
    "prev_app = prev_app.join(pd.get_dummies(prev_app.NAME_SELLER_INDUSTRY), rsuffix='Seller_Ind').drop(\"NAME_SELLER_INDUSTRY\", axis=1)\n",
    "prev_app = prev_app.join(pd.get_dummies(prev_app.NAME_YIELD_GROUP), rsuffix='_Name_Group').drop(\"NAME_YIELD_GROUP\", axis=1)\n",
    "prev_app = prev_app.join(pd.get_dummies(prev_app.PRODUCT_COMBINATION)).drop(\"PRODUCT_COMBINATION\", axis=1)\n",
    "prev_app = prev_app.join(pd.get_dummies(prev_app.FLAG_LAST_APPL_PER_CONTRACT)).drop(\"FLAG_LAST_APPL_PER_CONTRACT\", axis=1)\n",
    "\n",
    "prev_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app['SK_ID_PREV'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prev_app has the same number of unique SK_ID_PREV identifiers as there are rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Previous Applications file from csv\n",
    "pos_cash_balance = pd.read_csv('POS_CASH_BALANCE.csv', index_col=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous Applications - transforming categorical variables into Dummies\n",
    "\n",
    "pos_cash_balance = prev_app.join(pd.get_dummies(pos_cash_balance.NAME_CONTRACT_STATUS), \\\n",
    "                                 rsuffix='_Name_Status').drop(\"NAME_CONTRACT_STATUS\", axis=1)\n",
    "\n",
    "pos_cash_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_balance['SK_ID_PREV'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos cash balance has the same number of unique SK_ID_PREV identifiers as there are rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Installments Payments file from csv\n",
    "installments_payments = pd.read_csv('installments_payments.csv', index_col=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(installments_payments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating unique combinations of the previous loans IDs with the current loan IDs\n",
    "installments_payments_prev_curr = installments_payments[['SK_ID_PREV','SK_ID_CURR']]\n",
    "installments_payments_prev_curr = installments_payments_prev_curr.groupby(['SK_ID_PREV'], as_index=False).mean()\n",
    "installments_payments_prev_curr.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Current ID variable - later will re-merge\n",
    "installments_payments = installments_payments.drop(\"SK_ID_CURR\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Variable for a flag of a late payment relative to when it was supposed to have been paid\n",
    "installments_payments['late'] = installments_payments['DAYS_ENTRY_PAYMENT'] > installments_payments['DAYS_INSTALMENT']\n",
    "installments_payments['late'] = installments_payments['late'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing past Installement payment information and grouping by previous ID.\n",
    "installments_payments = installments_payments.groupby(['SK_ID_PREV'], as_index=False).sum()\n",
    "installments_payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments['SK_ID_PREV'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging back the unique SK_ID_CURR variable to the installments_payments DataFrame\n",
    "installments_payments = installments_payments.merge(installments_payments_prev_curr, on=('SK_ID_PREV'), how='left')\n",
    "installments_payments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installments payments now has a unique SK_ID_PREV for every row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Curr ID from merged files to keep a single column\n",
    "pos_cash_balance = pos_cash_balance.drop(['SK_ID_CURR'], axis=1)\n",
    "installments_payments = installments_payments.drop(['SK_ID_CURR'], axis=1)\n",
    "cc_balance = cc_balance.drop(['SK_ID_CURR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging prev_app with installments_payments\n",
    "prev_app_and_installments = prev_app.merge(installments_payments, on=('SK_ID_PREV'), how='left')\n",
    "prev_app_and_installments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging prev_app+installments_payments with pos_cash_balance\n",
    "prev_app_and_installments_pos_cash = prev_app_and_installments.merge(pos_cash_balance, on=('SK_ID_PREV'), how='left')\n",
    "prev_app_and_installments_pos_cash.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging prev_app+installments_payments+pos_cash_balance with cc_balance\n",
    "all_prev_data = prev_app_and_installments_pos_cash.merge(cc_balance, on=('SK_ID_PREV'), how='left')\n",
    "all_prev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that removal of Curr_ID Columns worked and we have a single Curr_ID Column for subsequent merging\n",
    "all_prev_data.columns = all_prev_data.columns.map(str)\n",
    "filter_col = all_prev_data.loc[:, all_prev_data.columns.str.startswith('SK_ID_CURR')]\n",
    "filter_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col['SK_ID_CURR'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prev_data['SK_ID_CURR'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing past Installement payment information and grouping by Current ID (Combining info on past transactions)\n",
    "all_prev_data_by_ID_CURR = all_prev_data.groupby(['SK_ID_CURR'], as_index=False).sum()\n",
    "all_prev_data_by_ID_CURR = all_prev_data_by_ID_CURR.drop(['SK_ID_PREV'], axis=1)\n",
    "all_prev_data_by_ID_CURR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prev_data_by_ID_CURR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Train Data Set\n",
    "application_train = pd.read_csv('application_train.csv', index_col=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging grouped previous transactions and bureau files with the train data set\n",
    "application_train_merged = application_train.merge(all_prev_data_by_ID_CURR, on=('SK_ID_CURR'), how='left')\n",
    "application_train_merged = application_train_merged.merge(bureau_grouped, on=('SK_ID_CURR'), how='left')\n",
    "application_train_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now looking at missing data to decide which columns to drop and which to fill in data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming a duplicate named variable\n",
    "cols = []\n",
    "count = 1\n",
    "for column in application_train_merged.columns:\n",
    "    if column == 'AMT_ANNUITY_x':\n",
    "        cols.append('AMT_ANNUITY_x_'+str(count))\n",
    "        count+=1\n",
    "        continue\n",
    "    cols.append(column)\n",
    "application_train_merged.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming a duplicate named variable\n",
    "cols = []\n",
    "count = 1\n",
    "for column in application_train_merged.columns:\n",
    "    if column == 'AMT_ANNUITY_y':\n",
    "        cols.append('AMT_ANNUITY_y_'+str(count))\n",
    "        count+=1\n",
    "        continue\n",
    "    cols.append(column)\n",
    "application_train_merged.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of missing variables by column \n",
    "\n",
    "def Missing_variables(df):\n",
    "    nan_values = []\n",
    "    for i in df:\n",
    "        count_nan = len(df[i]) - df[i].count()\n",
    "        nan_values.append(count_nan)\n",
    "    percentage = [x / len(df)*100 for x in nan_values]\n",
    "    nan_df = list(df.columns.values)\n",
    "    percentage_list = pd.DataFrame(\n",
    "    {'columns': nan_df,\n",
    "     'number_Nan': nan_values,\n",
    "     'percentage': percentage\n",
    "    })\n",
    "    \n",
    "    Nan_Ascending = percentage_list.loc[percentage_list['percentage'] != 0].sort_values('percentage', ascending=False)\n",
    "    \n",
    "    return Nan_Ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before removing variables with high Nan values, making exception to EXT_SOURCE_1\n",
    "# This is after seeing its high feature importance to the model\n",
    "application_train_merged['EXT_SOURCE_1'].fillna((application_train_merged['EXT_SOURCE_1'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_var = Missing_variables(application_train_merged)\n",
    "print('There are', len(Missing_var), 'columns with missing variables out of', \\\n",
    "      len(application_train_merged.columns), 'columns in the dataframe')\n",
    "Missing_var.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of column names for columns with over 50% of missing variables\n",
    "# Removing these columns from the application_train_merged data set\n",
    "\n",
    "Mising_var_high = Missing_var[Missing_var['percentage']>35]\n",
    "high_Nan_column_names = Mising_var_high['columns'].tolist()\n",
    "\n",
    "application_train_merged = application_train_merged.drop(high_Nan_column_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_var = Missing_variables(application_train_merged)\n",
    "print('There are', len(Missing_var), 'columns with missing variables out of', \\\n",
    "      len(application_train_merged.columns), 'columns in the dataframe')\n",
    "Missing_var.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The majority of missing values are 5.350703 percentage, and those appear on those that did not merge \n",
    "# with the applicaiton_train data.\n",
    "# Since there is much incomplete data - those 5% of the data will be removed for the model data-sets\n",
    "# Using \"late\" variable for the removal. \n",
    "\n",
    "application_train_merged = application_train_merged[np.isfinite(application_train_merged['late'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_var = Missing_variables(application_train_merged)\n",
    "print('There are', len(Missing_var), 'columns with missing variables out of', \\\n",
    "      len(application_train_merged.columns), 'columns in the dataframe')\n",
    "Missing_var.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next majority of missing values are 14.275554 percentage, and those appear on those that did not merge \n",
    "# with the applicaiton_train data from the Bureau data.\n",
    "# Since there is much incomplete data - those 14% of the data will be removed for the model data-sets\n",
    "# Using \"Closed\" variable for the removal. \n",
    "\n",
    "application_train_merged = application_train_merged[np.isfinite(application_train_merged['Closed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_var = Missing_variables(application_train_merged)\n",
    "print('There are', len(Missing_var), 'columns with missing variables out of', \\\n",
    "      len(application_train_merged.columns), 'columns in the dataframe')\n",
    "Missing_var.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NaN values in OCCUPATION_TYPE as \"Unemployed\"\n",
    "application_train_merged['OCCUPATION_TYPE'].fillna('Unemployed', inplace=True)\n",
    "\n",
    "# Filling NaN values in EXT_Source_3 and _2 with the mean of each.\n",
    "application_train_merged['EXT_SOURCE_3'].fillna((application_train_merged['EXT_SOURCE_3'].mean()), inplace=True)\n",
    "application_train_merged['EXT_SOURCE_2'].fillna((application_train_merged['EXT_SOURCE_2'].mean()), inplace=True)\n",
    "\n",
    "# Filling NaN values in NAME_TYPE_SUITE as \"Unaccompanied\"\n",
    "application_train_merged['NAME_TYPE_SUITE'].fillna('Unaccompanied', inplace=True)\n",
    "\n",
    "# Filling NaN values in XXX_CNT_SOCIAL_CIRCLE as \"0.0\". Making assumption that not listed is not observed.\n",
    "# These are defined as \"How many observation of client's social surroundings defaulted on 30 DPD (days past due)\"\"\n",
    "application_train_merged['OBS_30_CNT_SOCIAL_CIRCLE'].fillna('0.0', inplace=True)\n",
    "application_train_merged['DEF_30_CNT_SOCIAL_CIRCLE'].fillna('0.0', inplace=True)\n",
    "application_train_merged['OBS_60_CNT_SOCIAL_CIRCLE'].fillna('0.0', inplace=True)\n",
    "application_train_merged['DEF_60_CNT_SOCIAL_CIRCLE'].fillna('0.0', inplace=True)\n",
    "\n",
    "# Filling NaN values in EXT_Source_3 and _2 with the mean of each.\n",
    "application_train_merged['AMT_GOODS_PRICE'].fillna((application_train_merged['AMT_GOODS_PRICE'].mean()), inplace=True)\n",
    "application_train_merged['AMT_ANNUITY_x_1'].fillna((application_train_merged['AMT_ANNUITY_x_1'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_var = Missing_variables(application_train_merged)\n",
    "print('There are', len(Missing_var), 'columns with missing variables out of', \\\n",
    "      len(application_train_merged.columns), 'columns in the dataframe')\n",
    "Missing_var.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving to look at outlier values... Let's save & move to EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train_merged.to_csv('application_train_merged.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train_merged.TARGET.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
